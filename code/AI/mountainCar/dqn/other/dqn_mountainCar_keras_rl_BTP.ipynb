{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_mountainCar_keras_rl_BTP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzsWMISLruj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip uninstall tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kke2lDYtsSWa",
        "colab_type": "code",
        "outputId": "12a028cc-8ec2-487d-f580-0f60130fef4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 80kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.17.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.1.7)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.8.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ9Rv9zV0qbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "3e470c32-bf5d-47ef-9203-e0490e98ffc7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7ij71fPH_xB",
        "colab_type": "code",
        "outputId": "50f5f4b6-ac08-4128-c9e6-84059f0282ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "!pip install keras-rl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.3.1)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=1f13f1a75fdc9487088dc8c3cdb2a97d46917ecc6f9a124bc755a74bc05fa8d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRRAiAZ5Hyey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ede2d7d-bbdf-44ae-b434-0ed8c2b7098a"
      },
      "source": [
        "#===================== Import libraies=================#\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib as plt\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-flmoriH2vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#=========================== set env =================#\n",
        "ENV_NAME = 'MountainCar-v0'\n",
        "env = gym.make(ENV_NAME) # Get the environment and extract the number of actions available in the Cartpole problem\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLESEFq3IYoU",
        "colab_type": "code",
        "outputId": "15e5903d-6bd6-409e-ddf5-3896e82b7a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "#=================== Create model ======================\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 99\n",
            "Trainable params: 99\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPvisY5YP2G4",
        "colab_type": "code",
        "outputId": "d000abad-4486-4f69-fe8c-ab93492ced39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip install pyglet==1.3.2 #--force-reinstall\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5PTwv48I2Vl",
        "colab_type": "code",
        "outputId": "b91d6b6e-80c3-4b40-8ff8-eac8fce03220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "policy = EpsGreedyQPolicy()\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=20000, visualize=True, verbose=2)# visualize slows down training quite a lot. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Training for 20000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   200/20000: episode: 1, duration: 3.194s, episode steps: 200, steps per second: 63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000], mean observation: -0.265 [-0.599, 0.007], loss: 0.462561, mean_absolute_error: 0.451966, mean_q: 0.073036\n",
            "   400/20000: episode: 2, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.260 [-0.602, 0.007], loss: 0.441114, mean_absolute_error: 0.473975, mean_q: -0.041098\n",
            "   600/20000: episode: 3, duration: 1.189s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.261 [-0.566, 0.003], loss: 0.435431, mean_absolute_error: 0.512502, mean_q: -0.157210\n",
            "   800/20000: episode: 4, duration: 1.176s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000], mean observation: -0.263 [-0.658, 0.008], loss: 0.434326, mean_absolute_error: 0.562435, mean_q: -0.278324\n",
            "  1000/20000: episode: 5, duration: 1.185s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.595 [0.000, 2.000], mean observation: -0.294 [-0.920, 0.032], loss: 0.431688, mean_absolute_error: 0.624547, mean_q: -0.402519\n",
            "  1200/20000: episode: 6, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.710 [0.000, 2.000], mean observation: -0.292 [-0.939, 0.035], loss: 0.425327, mean_absolute_error: 0.706187, mean_q: -0.539847\n",
            "  1400/20000: episode: 7, duration: 1.183s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000], mean observation: -0.264 [-0.637, 0.011], loss: 0.422329, mean_absolute_error: 0.795577, mean_q: -0.679544\n",
            "  1600/20000: episode: 8, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000], mean observation: -0.261 [-0.557, 0.004], loss: 0.418106, mean_absolute_error: 0.876257, mean_q: -0.825250\n",
            "  1800/20000: episode: 9, duration: 1.141s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000], mean observation: -0.280 [-0.920, 0.032], loss: 0.413485, mean_absolute_error: 0.960978, mean_q: -0.973070\n",
            "  2000/20000: episode: 10, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.219 [-0.990, 0.108], loss: 0.402512, mean_absolute_error: 1.059366, mean_q: -1.125356\n",
            "  2200/20000: episode: 11, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.425 [0.000, 2.000], mean observation: -0.296 [-0.806, 0.024], loss: 0.383464, mean_absolute_error: 1.176472, mean_q: -1.304328\n",
            "  2400/20000: episode: 12, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.281 [-0.869, 0.029], loss: 0.359671, mean_absolute_error: 1.346619, mean_q: -1.567636\n",
            "  2600/20000: episode: 13, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.262 [-0.590, 0.006], loss: 0.359798, mean_absolute_error: 1.514598, mean_q: -1.820995\n",
            "  2800/20000: episode: 14, duration: 1.192s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000], mean observation: -0.279 [-0.797, 0.024], loss: 0.357100, mean_absolute_error: 1.680501, mean_q: -2.075985\n",
            "  3000/20000: episode: 15, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000], mean observation: -0.287 [-0.810, 0.021], loss: 0.352714, mean_absolute_error: 1.857941, mean_q: -2.346483\n",
            "  3200/20000: episode: 16, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000], mean observation: -0.322 [-0.723, 0.007], loss: 0.327876, mean_absolute_error: 2.064774, mean_q: -2.652919\n",
            "  3400/20000: episode: 17, duration: 1.174s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.545 [0.000, 2.000], mean observation: -0.303 [-0.973, 0.036], loss: 0.321761, mean_absolute_error: 2.286511, mean_q: -2.989872\n",
            "  3600/20000: episode: 18, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000], mean observation: -0.273 [-0.700, 0.016], loss: 0.320222, mean_absolute_error: 2.519508, mean_q: -3.334387\n",
            "  3800/20000: episode: 19, duration: 1.177s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.540 [0.000, 2.000], mean observation: -0.290 [-0.700, 0.010], loss: 0.309527, mean_absolute_error: 2.752014, mean_q: -3.690070\n",
            "  4000/20000: episode: 20, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.320 [0.000, 2.000], mean observation: -0.306 [-0.761, 0.014], loss: 0.295215, mean_absolute_error: 3.001287, mean_q: -4.078782\n",
            "  4200/20000: episode: 21, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.375 [0.000, 2.000], mean observation: -0.306 [-0.851, 0.022], loss: 0.292316, mean_absolute_error: 3.261769, mean_q: -4.481772\n",
            "  4400/20000: episode: 22, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.625 [0.000, 2.000], mean observation: -0.296 [-0.747, 0.020], loss: 0.304103, mean_absolute_error: 3.526515, mean_q: -4.890200\n",
            "  4600/20000: episode: 23, duration: 1.130s, episode steps: 200, steps per second: 177, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000], mean observation: -0.287 [-0.685, 0.016], loss: 0.294438, mean_absolute_error: 3.805469, mean_q: -5.319434\n",
            "  4800/20000: episode: 24, duration: 1.175s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.620 [0.000, 2.000], mean observation: -0.289 [-0.710, 0.015], loss: 0.301616, mean_absolute_error: 4.107668, mean_q: -5.771600\n",
            "  5000/20000: episode: 25, duration: 1.177s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000], mean observation: -0.278 [-0.596, 0.009], loss: 0.314341, mean_absolute_error: 4.396927, mean_q: -6.213218\n",
            "  5200/20000: episode: 26, duration: 1.145s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000], mean observation: -0.272 [-0.719, 0.020], loss: 0.293803, mean_absolute_error: 4.691719, mean_q: -6.665861\n",
            "  5400/20000: episode: 27, duration: 1.171s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000], mean observation: -0.274 [-0.584, 0.008], loss: 0.332710, mean_absolute_error: 5.017081, mean_q: -7.152550\n",
            "  5600/20000: episode: 28, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000], mean observation: -0.290 [-0.746, 0.018], loss: 0.359712, mean_absolute_error: 5.325361, mean_q: -7.608550\n",
            "  5800/20000: episode: 29, duration: 1.152s, episode steps: 200, steps per second: 174, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.580 [0.000, 2.000], mean observation: -0.288 [-0.774, 0.021], loss: 0.323298, mean_absolute_error: 5.646105, mean_q: -8.099898\n",
            "  6000/20000: episode: 30, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000], mean observation: -0.287 [-0.670, 0.013], loss: 0.347808, mean_absolute_error: 5.962916, mean_q: -8.581086\n",
            "  6200/20000: episode: 31, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.268 [-0.649, 0.012], loss: 0.364899, mean_absolute_error: 6.304125, mean_q: -9.098117\n",
            "  6400/20000: episode: 32, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.265 [-0.704, 0.021], loss: 0.357022, mean_absolute_error: 6.631960, mean_q: -9.594510\n",
            "  6600/20000: episode: 33, duration: 1.014s, episode steps: 200, steps per second: 197, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.550 [0.000, 2.000], mean observation: -0.294 [-0.673, 0.011], loss: 0.400414, mean_absolute_error: 6.993823, mean_q: -10.136127\n",
            "  6800/20000: episode: 34, duration: 1.050s, episode steps: 200, steps per second: 190, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.258 [-0.692, 0.019], loss: 0.415998, mean_absolute_error: 7.326721, mean_q: -10.638694\n",
            "  7000/20000: episode: 35, duration: 1.186s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000], mean observation: -0.253 [-0.661, 0.015], loss: 0.442497, mean_absolute_error: 7.673463, mean_q: -11.163692\n",
            "  7200/20000: episode: 36, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000], mean observation: -0.272 [-0.741, 0.019], loss: 0.486131, mean_absolute_error: 8.030700, mean_q: -11.698982\n",
            "  7400/20000: episode: 37, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000], mean observation: -0.292 [-0.668, 0.010], loss: 0.497470, mean_absolute_error: 8.389324, mean_q: -12.235622\n",
            "  7600/20000: episode: 38, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000], mean observation: -0.275 [-0.643, 0.018], loss: 0.524887, mean_absolute_error: 8.758916, mean_q: -12.796394\n",
            "  7800/20000: episode: 39, duration: 1.158s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.239 [-0.576, 0.015], loss: 0.556893, mean_absolute_error: 9.114462, mean_q: -13.325603\n",
            "  8000/20000: episode: 40, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.515 [0.000, 2.000], mean observation: -0.225 [-0.496, 0.007], loss: 0.669524, mean_absolute_error: 9.453691, mean_q: -13.835958\n",
            "  8200/20000: episode: 41, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000], mean observation: -0.237 [-0.596, 0.016], loss: 0.658773, mean_absolute_error: 9.816922, mean_q: -14.382655\n",
            "  8400/20000: episode: 42, duration: 1.173s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000], mean observation: -0.248 [-0.598, 0.016], loss: 0.650321, mean_absolute_error: 10.171994, mean_q: -14.926990\n",
            "  8600/20000: episode: 43, duration: 1.137s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000], mean observation: -0.227 [-0.528, 0.008], loss: 0.724086, mean_absolute_error: 10.521999, mean_q: -15.445446\n",
            "  8800/20000: episode: 44, duration: 1.168s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.253 [-0.561, 0.009], loss: 0.519060, mean_absolute_error: 10.889509, mean_q: -16.023851\n",
            "  9000/20000: episode: 45, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.260 [-0.665, 0.015], loss: 0.627537, mean_absolute_error: 11.274963, mean_q: -16.599754\n",
            "  9200/20000: episode: 46, duration: 1.194s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000], mean observation: -0.277 [-0.705, 0.015], loss: 0.850580, mean_absolute_error: 11.676179, mean_q: -17.183792\n",
            "  9400/20000: episode: 47, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000], mean observation: -0.257 [-0.565, 0.010], loss: 0.728056, mean_absolute_error: 12.067474, mean_q: -17.790306\n",
            "  9600/20000: episode: 48, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.231 [-0.547, 0.010], loss: 0.996550, mean_absolute_error: 12.439749, mean_q: -18.322058\n",
            "  9800/20000: episode: 49, duration: 1.140s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.244 [-0.550, 0.008], loss: 0.892085, mean_absolute_error: 12.790352, mean_q: -18.872681\n",
            " 10000/20000: episode: 50, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.255 [-0.613, 0.011], loss: 0.949335, mean_absolute_error: 13.171737, mean_q: -19.440605\n",
            " 10200/20000: episode: 51, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.245 [-0.555, 0.012], loss: 1.118772, mean_absolute_error: 13.555842, mean_q: -20.006298\n",
            " 10400/20000: episode: 52, duration: 1.169s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.590 [0.000, 2.000], mean observation: -0.218 [-0.553, 0.012], loss: 1.073329, mean_absolute_error: 13.886330, mean_q: -20.491325\n",
            " 10600/20000: episode: 53, duration: 1.134s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.735 [0.000, 2.000], mean observation: -0.214 [-0.525, 0.012], loss: 1.042502, mean_absolute_error: 14.228411, mean_q: -21.014694\n",
            " 10800/20000: episode: 54, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000], mean observation: -0.200 [-0.471, 0.006], loss: 0.881089, mean_absolute_error: 14.580173, mean_q: -21.542124\n",
            " 11000/20000: episode: 55, duration: 1.185s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.825 [0.000, 2.000], mean observation: -0.201 [-0.565, 0.013], loss: 1.040567, mean_absolute_error: 14.943115, mean_q: -22.088997\n",
            " 11200/20000: episode: 56, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.895 [0.000, 2.000], mean observation: -0.202 [-0.481, 0.005], loss: 1.522193, mean_absolute_error: 15.256804, mean_q: -22.529472\n",
            " 11400/20000: episode: 57, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.560 [0.000, 2.000], mean observation: -0.225 [-0.641, 0.021], loss: 1.236895, mean_absolute_error: 15.564009, mean_q: -23.012503\n",
            " 11600/20000: episode: 58, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.900 [0.000, 2.000], mean observation: -0.195 [-0.534, 0.012], loss: 1.373227, mean_absolute_error: 15.914299, mean_q: -23.529512\n",
            " 11800/20000: episode: 59, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.760 [0.000, 2.000], mean observation: -0.210 [-0.518, 0.008], loss: 1.266319, mean_absolute_error: 16.223083, mean_q: -24.005199\n",
            " 12000/20000: episode: 60, duration: 1.167s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.910 [0.000, 2.000], mean observation: -0.197 [-0.476, 0.006], loss: 1.250128, mean_absolute_error: 16.580542, mean_q: -24.556124\n",
            " 12200/20000: episode: 61, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000], mean observation: -0.195 [-0.542, 0.012], loss: 1.552045, mean_absolute_error: 16.926033, mean_q: -25.036766\n",
            " 12400/20000: episode: 62, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.925 [0.000, 2.000], mean observation: -0.196 [-0.489, 0.008], loss: 1.406145, mean_absolute_error: 17.206017, mean_q: -25.462431\n",
            " 12600/20000: episode: 63, duration: 1.184s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 2.000], mean observation: -0.197 [-0.427, 0.003], loss: 1.451934, mean_absolute_error: 17.502562, mean_q: -25.920410\n",
            " 12800/20000: episode: 64, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000], mean observation: -0.252 [-0.671, 0.021], loss: 1.596972, mean_absolute_error: 17.914976, mean_q: -26.551714\n",
            " 13000/20000: episode: 65, duration: 1.179s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.245 [-0.586, 0.016], loss: 1.300727, mean_absolute_error: 18.202148, mean_q: -27.001604\n",
            " 13200/20000: episode: 66, duration: 1.062s, episode steps: 200, steps per second: 188, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.660 [0.000, 2.000], mean observation: -0.215 [-0.488, 0.007], loss: 1.271098, mean_absolute_error: 18.588739, mean_q: -27.594788\n",
            " 13400/20000: episode: 67, duration: 0.999s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.244 [-0.609, 0.013], loss: 1.482427, mean_absolute_error: 18.934267, mean_q: -28.111198\n",
            " 13600/20000: episode: 68, duration: 1.020s, episode steps: 200, steps per second: 196, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.605 [0.000, 2.000], mean observation: -0.221 [-0.538, 0.010], loss: 2.635792, mean_absolute_error: 19.323549, mean_q: -28.635532\n",
            " 13800/20000: episode: 69, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.236 [-0.630, 0.018], loss: 2.419374, mean_absolute_error: 19.605801, mean_q: -29.071587\n",
            " 14000/20000: episode: 70, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 2.000], mean observation: -0.225 [-0.609, 0.019], loss: 2.273196, mean_absolute_error: 19.889854, mean_q: -29.489206\n",
            " 14200/20000: episode: 71, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000], mean observation: -0.238 [-0.712, 0.024], loss: 2.451506, mean_absolute_error: 20.194559, mean_q: -29.945364\n",
            " 14400/20000: episode: 72, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000], mean observation: -0.238 [-0.606, 0.018], loss: 1.933524, mean_absolute_error: 20.479586, mean_q: -30.406578\n",
            " 14600/20000: episode: 73, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.830 [0.000, 2.000], mean observation: -0.207 [-0.507, 0.010], loss: 2.494597, mean_absolute_error: 20.778414, mean_q: -30.819677\n",
            " 14800/20000: episode: 74, duration: 0.951s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 2.000], mean observation: -0.209 [-0.538, 0.013], loss: 2.657074, mean_absolute_error: 21.034796, mean_q: -31.208521\n",
            " 15000/20000: episode: 75, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000], mean observation: -0.250 [-0.620, 0.012], loss: 1.843157, mean_absolute_error: 21.339209, mean_q: -31.724846\n",
            " 15200/20000: episode: 76, duration: 1.159s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.670 [0.000, 2.000], mean observation: -0.217 [-0.501, 0.009], loss: 3.068115, mean_absolute_error: 21.664473, mean_q: -32.150276\n",
            " 15400/20000: episode: 77, duration: 1.161s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.575 [0.000, 2.000], mean observation: -0.223 [-0.556, 0.009], loss: 1.578840, mean_absolute_error: 21.938173, mean_q: -32.617710\n",
            " 15600/20000: episode: 78, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.253 [-0.673, 0.013], loss: 2.352327, mean_absolute_error: 22.248266, mean_q: -33.064587\n",
            " 15800/20000: episode: 79, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.510 [0.000, 2.000], mean observation: -0.232 [-0.588, 0.015], loss: 2.722705, mean_absolute_error: 22.585909, mean_q: -33.558781\n",
            " 16000/20000: episode: 80, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.760 [0.000, 2.000], mean observation: -0.204 [-0.574, 0.014], loss: 3.630795, mean_absolute_error: 22.844532, mean_q: -33.893456\n",
            " 16200/20000: episode: 81, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.875 [0.000, 2.000], mean observation: -0.201 [-0.450, 0.005], loss: 1.885209, mean_absolute_error: 23.081175, mean_q: -34.318272\n",
            " 16400/20000: episode: 82, duration: 1.172s, episode steps: 200, steps per second: 171, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.238 [-0.616, 0.007], loss: 2.956216, mean_absolute_error: 23.394444, mean_q: -34.770363\n",
            " 16600/20000: episode: 83, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.256 [-0.690, 0.018], loss: 2.769444, mean_absolute_error: 23.624487, mean_q: -35.109711\n",
            " 16800/20000: episode: 84, duration: 1.166s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.253 [-0.597, 0.010], loss: 3.109916, mean_absolute_error: 23.939869, mean_q: -35.590591\n",
            " 17000/20000: episode: 85, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.755 [0.000, 2.000], mean observation: -0.208 [-0.497, 0.009], loss: 2.221345, mean_absolute_error: 24.193605, mean_q: -35.991566\n",
            " 17200/20000: episode: 86, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000], mean observation: -0.266 [-0.675, 0.019], loss: 3.017588, mean_absolute_error: 24.467236, mean_q: -36.392323\n",
            " 17400/20000: episode: 87, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.242 [-0.714, 0.016], loss: 2.490981, mean_absolute_error: 24.762280, mean_q: -36.843021\n",
            " 17600/20000: episode: 88, duration: 1.145s, episode steps: 200, steps per second: 175, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.515 [0.000, 2.000], mean observation: -0.293 [-0.726, 0.013], loss: 3.847485, mean_absolute_error: 25.047085, mean_q: -37.220676\n",
            " 17800/20000: episode: 89, duration: 1.147s, episode steps: 200, steps per second: 174, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.415 [0.000, 2.000], mean observation: -0.305 [-0.723, 0.012], loss: 2.378120, mean_absolute_error: 25.286247, mean_q: -37.615421\n",
            " 18000/20000: episode: 90, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000], mean observation: -0.340 [-0.926, 0.023], loss: 2.157607, mean_absolute_error: 25.577398, mean_q: -38.063179\n",
            " 18200/20000: episode: 91, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.540 [0.000, 2.000], mean observation: -0.290 [-0.907, 0.027], loss: 4.268857, mean_absolute_error: 25.840342, mean_q: -38.388992\n",
            " 18400/20000: episode: 92, duration: 1.174s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000], mean observation: -0.226 [-0.540, 0.008], loss: 4.010421, mean_absolute_error: 26.128246, mean_q: -38.824337\n",
            " 18600/20000: episode: 93, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000], mean observation: -0.248 [-0.607, 0.014], loss: 3.888800, mean_absolute_error: 26.300892, mean_q: -39.091923\n",
            " 18800/20000: episode: 94, duration: 1.163s, episode steps: 200, steps per second: 172, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.252 [-0.701, 0.026], loss: 3.068171, mean_absolute_error: 26.518345, mean_q: -39.461811\n",
            " 19000/20000: episode: 95, duration: 1.177s, episode steps: 200, steps per second: 170, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000], mean observation: -0.231 [-0.508, 0.009], loss: 3.657016, mean_absolute_error: 26.754414, mean_q: -39.805508\n",
            " 19200/20000: episode: 96, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.239 [-0.640, 0.020], loss: 4.549259, mean_absolute_error: 27.008091, mean_q: -40.144325\n",
            " 19400/20000: episode: 97, duration: 1.101s, episode steps: 200, steps per second: 182, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.635 [0.000, 2.000], mean observation: -0.218 [-0.684, 0.023], loss: 3.517768, mean_absolute_error: 27.149864, mean_q: -40.365631\n",
            " 19600/20000: episode: 98, duration: 1.133s, episode steps: 200, steps per second: 176, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.234 [-0.654, 0.013], loss: 3.789410, mean_absolute_error: 27.400827, mean_q: -40.758690\n",
            " 19800/20000: episode: 99, duration: 1.147s, episode steps: 200, steps per second: 174, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.262 [-0.580, 0.006], loss: 2.693799, mean_absolute_error: 27.578964, mean_q: -41.050613\n",
            " 20000/20000: episode: 100, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.252 [-0.648, 0.012], loss: 3.130582, mean_absolute_error: 27.877680, mean_q: -41.500343\n",
            "done, took 117.805 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-eGVVjv2J5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rtrbtW427_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['episode_reward'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIWK1u51yhoK",
        "colab_type": "code",
        "outputId": "7b36a2fd-6ca0-4aae-cb55-cb98cf24a8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model.summary()\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 99\n",
            "Trainable params: 99\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAIjCAYAAADbSV1+AAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nY2oixTDURIphqIkU4wOAP9JNpJD/B5ZyRuzBuMauAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyq2yBdsJQmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn.test(env, nb_episodes=100, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkvMyGi-9Pel",
        "colab_type": "text"
      },
      "source": [
        "#===================================== end =============================#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WbKRnQ-LTSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#======================================== simple Random walk ========================================================#\n",
        "# !pip install --force-reinstall pyglet==1.3.2\n",
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(5):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "  if done:\n",
        "    break\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnumdDG6L074",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}